{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "# eval_interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lenth of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249892\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 2000 character of the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Dorothy and the Wizard in Oz\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: Dorothy and the Wizard in Oz\n",
      "\n",
      "Author: L. Frank Baum\n",
      "\n",
      "Release date: February 1, 1996 [eBook #420]\n",
      "                Most recently updated: January 1, 2021\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Produced by Dennis Amundson.\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK DOROTHY AND THE WIZARD IN OZ ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Dennis Amundson.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dorothy and the Wizard in Oz\n",
      "\n",
      "\n",
      "  A Faithful Record of Their Amazing Adventures\n",
      "    in an Underground World; and How with the\n",
      "     Aid of Their Friends Zeb Hugson, Eureka\n",
      "       the Kitten, and Jim the Cab-Horse,\n",
      "            They Finally Reached the\n",
      "                Wonderful Land\n",
      "                    of Oz\n",
      "\n",
      "\n",
      "by\n",
      "\n",
      "L. Frank Baum\n",
      "\n",
      "\"Royal Historian of Oz\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   --To My Readers--\n",
      "   1.  The Earthquake\n",
      "   2.  The Glass City\n",
      "   3.  The Arrival of the Wizard\n",
      "   4.  The Vegetable Kingdom\n",
      "   5.  Dorothy Picks the Princess\n",
      "   6.  The Mangaboos Prove Dangerous\n",
      "   7.  Into the Black Pit and Out Again\n",
      "   8.  The Valley of Voices\n",
      "   9.  They Fight the Invisible Bears\n",
      "  10.  The Braided Man of Pyramid Mountain\n",
      "  11.  They Meet the Wooden Gargoyles\n",
      "  12.  A Wonderful Escape\n",
      "  13.  The Den of the Dragonettes\n",
      "  14.  Ozma Uses the Magic Belt\n",
      "  15.  Old Friends are Reunited\n",
      "  16.  Jim, the Cab-Horse\n",
      "  17.  The Nine Tiny Piglets\n",
      "  18.  The Trial of Eureka, the Kitten\n",
      "  19.  The Wizard Performs Another Trick\n",
      "  20.  Zeb Returns to the Ranch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To My Readers\n",
      "\n",
      "\n",
      "It's no use; no use at all.  The children won't let me stop telling\n",
      "tales of the Land of Oz.  I know lots of other stories, and I hope to\n",
      "tell them, some time or another; but just now my loving tyrants won't\n",
      "allow me.  They cry: \"Oz--Oz! more about Oz, Mr. Baum!\" and what can I\n",
      "do but obey their commands?\n",
      "\n",
      "This is Our Book--mine and the children's.  For they have flooded me\n",
      "with thousands of suggestions in regard to it, and I have honestly\n",
      "tried to adopt as many of these suggestions as could be fitted into one\n",
      "story.\n",
      "\n",
      "After the wonderful success of \"Ozma of Oz\" it is evident that Dorothy\n",
      "has become a firm fixture in these Oz stories.  The little ones all\n",
      "love Dorothy, and as one of my small friends aptly states: \"It isn't a\n",
      "real Oz story without her.\"  So here she is again, as sweet and gentle\n",
      "and innocent as ever, I hope, and the heroine of another strange\n",
      "adventure.\n",
      "\n",
      "There were many requests from my little correspondents for \"more about\n",
      "the Wizard.\"  It seems the jolly old fellow made hosts of friends in\n",
      "the first Oz book, in spite of the fact that he frankly acknowledged\n",
      "himself \"a humbug.\"  The children had heard how he mounted into the sky\n",
      "in a balloon and they were all waiting for him to come down again.  So\n",
      "what could I do but tell \"what happened to the Wizard afterward\"?  You\n",
      "will find him in these pages, just the same humbug Wizard as before.\n",
      "\n",
      "There was one thing the children demanded which I found it impossible\n",
      "to do in this present book: they bade me introduce Toto, Dorothy's\n",
      "little black dog, who has many friends among my readers.  But you will\n",
      "see, when you begin to read the story, that Toto was in Kansas while\n",
      "Dorothy was in California, and so she had to start on her adventure\n",
      "without him.  In this book Dorothy had to take her kitten with her\n",
      "instead of her dog; but in the next Oz book, if I am permitted to write\n",
      "one, I intend to tell a good deal about Toto's further history.\n",
      "\n",
      "Princess Ozma, whom I love as much as my readers do, is again\n",
      "introduced in this story, and so are several of our old friends of Oz.\n",
      "You will also become acquainted with Jim the Cab-Horse, the Nine Tiny\n",
      "Piglets, and Eureka, the Kitten.  I am sorry the kitten was not as well\n",
      "behaved as she ought to have been; but perhaps she wasn't brought up\n",
      "properly.  Dorothy found her, you see, and who her parents were nobody\n",
      "knows.\n",
      "\n",
      "I believe, my dears, that I am the proudest story-teller that ever\n",
      "lived.  Many a time tears of pride and joy have stood in my eyes while\n",
      "I read the tender, loving, appealing letters that came to me in almost\n",
      "every mail from my little readers.  To have pleased you, to have\n",
      "interested you, to have won your friendship, and perhaps your love,\n",
      "through my stories, is to my mind as great an achievement as to become\n",
      "President of the United States.  Indeed, I would much rather be your\n",
      "story-teller, under these conditions, than to be the President.  So you\n",
      "have helped me to fulfill my life's ambition, and I am more grateful to\n",
      "you, my dears, than I can express in words.\n",
      "\n",
      "I try to answer every letter of my young correspondents; yet sometimes\n",
      "there are so many letters that a little time must pass before you get\n",
      "your answer.  But be patient, friends, for the answer will surely come,\n",
      "and by writing to me you more than repay me for the pleasant task of\n",
      "preparing these books.  Besides, I am proud to acknowledge that the\n",
      "books are partly yours, for your suggestions often guide me in telling\n",
      "the stories, and I am sure they would not be half so good without your\n",
      "clever and thoughtful assistance.\n",
      "\n",
      "L. FRANK BAUM\n",
      "\n",
      "Coronado, 1908.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.  The Earthquake\n",
      "\n",
      "\n",
      "The train from 'Frisco was very late.  It should have arrived at\n",
      "Hugson's Siding at midnight, but it was already five o'clock and the\n",
      "gray dawn was breaking in the east when the little train slowly rumbled\n",
      "up to the open shed that served for the station-house.  As it came to a\n",
      "stop the conductor called out in a loud voice:\n",
      "\n",
      "\"Hugson's Siding!\"\n",
      "\n",
      "At once a little girl rose from her seat and walked to the door of the\n",
      "car, carrying a wicker suit-case in one hand and a round bird-cage\n",
      "covered up with newspapers in the other, while a parasol was tucked\n",
      "under her arm.  The conductor helped her off the car and then the\n",
      "engineer started his train again, so that it puffed and groaned and\n",
      "moved slowly away up the track.  The reason he was so late was because\n",
      "all through the night there were times when the solid earth shook and\n",
      "trembled under him, and the engineer was afraid that at any moment the\n",
      "rails might spread apart and an accident happen to his passengers.  So\n",
      "he moved the cars slowly and with caution.\n",
      "\n",
      "The little girl stood still to watch until the train had disappeared\n",
      "around a curve; then she turned to see where she was.\n",
      "\n",
      "The shed at Hugson's Siding was bare save for an old wooden bench, and\n",
      "did not look very inviting.  As she peered through the soft gray light\n",
      "not a house of any sort was visible near the station, nor was any\n",
      "person in sight; but after a while the child discovered a horse and\n",
      "buggy standing near a group of trees a short distance away.  She walked\n",
      "toward it and found the horse tied to a tree and standing motionless,\n",
      "with its head hanging down almost to the ground.  It was a big horse,\n",
      "tall and bony, with long legs and large knees and feet.  She could\n",
      "count his ribs easily where they showed through the skin of his body,\n",
      "and his head was long and seemed altogether too big for him, as if it\n",
      "did not fit.  His tail was short and scraggly, and his harness had been\n",
      "broken in many places and fastened together again with cords and bits\n",
      "of wire.  The buggy seemed almost new, for it had a shiny top and side\n",
      "curtains.  Getting around in front, so that she could look inside, the\n",
      "girl saw a boy curled up on the seat, fast asleep.\n",
      "\n",
      "She set down the bird-cage and poked the boy with her parasol.\n",
      "Presently he woke up, rose to a sitting position and rubbed his eyes\n",
      "briskly.\n",
      "\n",
      "\"Hello!\" he said, seeing her, \"are you Dorothy Gale?\"\n",
      "\n",
      "\"Yes,\" she answered, looking gravely at his tousled hair and blinking\n",
      "gray eyes.  \"Have you come to take me to Hugson's Ranch?\"\n",
      "\n",
      "\"Of course,\" he answered.  \"Train in?\"\n",
      "\n",
      "\"I couldn't be here if it wasn't,\" she said.\n",
      "\n",
      "He laughed at that, and his laugh was merry and frank.  Jumping out of\n",
      "the buggy he put Dorothy's suit-case under the seat and her bird-cage\n",
      "on the floor in front.\n",
      "\n",
      "\"Canary-birds?\" he asked.\n",
      "\n",
      "\"Oh no; it's just Eureka, my kitten.  I thought that was the best way\n",
      "to carry her.\"\n",
      "\n",
      "The boy nodded.\n",
      "\n",
      "\"Eureka's a funny name for a cat,\" he remarked.\n",
      "\n",
      "\"I named my kitten that because I found it,\" she explained.  \"Uncle\n",
      "Henry says 'Eureka' means 'I have found it.'\"\n",
      "\n",
      "\"All right; hop in.\"\n",
      "\n",
      "She climbed into the buggy and he followed her.  Then the boy picked up\n",
      "the reins, shook them, and said \"Gid-dap!\"\n",
      "\n",
      "The horse did not stir.  Dorothy thought he just wiggled one of his\n",
      "drooping ears, but that was all.\n",
      "\n",
      "\"Gid-dap!\" called the boy, again.\n",
      "\n",
      "The horse stood still.\n",
      "\n",
      "\"Perhaps,\" said Dorothy, \"if you untied him, he would go.\"\n",
      "\n",
      "The boy laughed cheerfully and jumped out.\n",
      "\n",
      "\"Guess I'm half asleep yet,\" he said, untying the horse.  \"But Jim\n",
      "knows his business all right--don't you, Jim?\" patting the long nose of\n",
      "the animal.\n",
      "\n",
      "Then he got into the buggy again and took the reins, and the horse at\n",
      "once backed away from the tree, turned slowly around, and began to trot\n",
      "down the sandy road which was just visible in the dim light.\n",
      "\n",
      "\"Thought that train would never come,\" observed the boy.  \"I've waited\n",
      "at that station for five hours.\"\n",
      "\n",
      "\"We had a lot of earthquakes,\" said Dorothy.  \"Didn't you feel the\n",
      "ground shake?\"\n",
      "\n",
      "\"Yes; but we're used to such things in California,\" he replied.  \"They\n",
      "don't scare us much.\"\n",
      "\n",
      "\"The conductor said it was the worst quake he ever knew.\"\n",
      "\n",
      "\"Did he?  Then it must have happened while I was asleep,\" he said\n",
      "thoughtfully.\n",
      "\n",
      "\"How is Uncle Henry?\" she enquired, after a pause during which the\n",
      "horse continued to trot with long, regular strides.\n",
      "\n",
      "\"He's pretty well.  He and Uncle Hugson have been having a fine visit.\"\n",
      "\n",
      "\"Is Mr. Hugson your uncle?\" she asked.\n",
      "\n",
      "\"Yes.  Uncle Bill Hugson married your Uncle Henry's wife's sister; so\n",
      "we must be second cousins,\" said the boy, in an amused tone.  \"I work\n",
      "for Uncle Bill on his ranch, and he pays me six dollars a month and my\n",
      "board.\"\n",
      "\n",
      "\"Isn't that a great deal?\" she asked, doubtfully.\n",
      "\n",
      "\"Why, it's a great deal for Uncle Hugson, but not for me.  I'm a\n",
      "splendid worker.  I work as well as I sleep,\" he added, with a laugh.\n",
      "\n",
      "\"What is your name?\" said Dorothy, thinking she liked the boy's manner\n",
      "and the cheery tone of his voice.\n",
      "\n",
      "\"Not a very pretty one,\" he answered, as if a little ashamed.  \"My\n",
      "whole name is Zebediah; but folks just call me 'Zeb.'  You've been to\n",
      "Australia, haven't you?\"\n",
      "\n",
      "\"Yes; with Uncle Henry,\" she answered.  \"We got to San Francisco a week\n",
      "ago, and Uncle Henry went right on to Hugson's Ranch for a visit while\n",
      "I stayed a few days in the city with some friends we had met.\"\n",
      "\n",
      "\"How long will you be with us?\" he asked.\n",
      "\n",
      "\"Only a day.  Tomorrow Uncle Henry and I must start back for Kansas.\n",
      "We've been away for a long time, you know, and so we're anxious to get\n",
      "home again.\"\n",
      "\n",
      "The boy flicked the big, boney horse with his whip and looked\n",
      "thoughtful.  Then he started to say something to his little companion,\n",
      "but before he could speak the buggy began to sway dangerously from side\n",
      "to side and the earth seemed to rise up before them.  Next minute there\n",
      "was a roar and a sharp crash, and at her side Dorothy saw the ground\n",
      "open in a wide crack and then come together again.\n",
      "\n",
      "\"Goodness!\" she cried, grasping the iron rail of the seat.  \"What was\n",
      "that?\"\n",
      "\n",
      "\"That was an awful big quake,\" replied Zeb, with a white face.  \"It\n",
      "almost got us that time, Dorothy.\"\n",
      "\n",
      "The horse had stopped short, and stood firm as a rock.  Zeb shook the\n",
      "reins and urged him to go, but Jim was stubborn.  Then the boy cracked\n",
      "his whip and touched the animal's flanks with it, and after a low moan\n",
      "of protest Jim stepped slowly along the road.\n",
      "\n",
      "Neither the boy nor the girl spoke again for some minutes.  There was a\n",
      "breath of danger in the very air, and every few moments the earth would\n",
      "shake violently.  Jim's ears were standing erect upon his head and\n",
      "every muscle of his big body was tense as he trotted toward home.  He\n",
      "was not going very fast, but on his flanks specks of foam began to\n",
      "appear and at times he would tremble like a leaf.\n",
      "\n",
      "The sky had grown darker again and the wind made queer sobbing sounds\n",
      "as it swept over the valley.\n",
      "\n",
      "Suddenly there was a rending, tearing sound, and the earth split into\n",
      "another great crack just beneath the spot where the horse was standing.\n",
      "With a wild neigh of terror the animal fell bodily into the pit,\n",
      "drawing the buggy and its occupants after him.\n",
      "\n",
      "Dorothy grabbed fast hold of the buggy top and the boy did the same.\n",
      "The sudden rush into space confused them so that they could not think.\n",
      "\n",
      "Blackness engulfed them on every side, and in breathless silence they\n",
      "waited for the fall to end and crush them against jagged rocks or for\n",
      "the earth to close in on them again and bury them forever in its\n",
      "dreadful depths.\n",
      "\n",
      "The horrible sensation of falling, the darkness and the terrifying\n",
      "noises, proved more than Dorothy could endure and for a few moments the\n",
      "little girl lost consciousness.  Zeb, being a boy, did not faint, but\n",
      "he was badly frightened, and clung to the buggy seat with a tight grip,\n",
      "expecting every moment would be his last.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.  The Glass City\n",
      "\n",
      "\n",
      "When Dorothy recovered her senses they were still falling, but not so\n",
      "fast.  The top of the buggy caught the air like a parachute or an\n",
      "umbrella filled with wind, and held them back so that they floated\n",
      "downward with a gentle motion that was not so very disagreeable to\n",
      "bear.  The worst thing was their terror of reaching the bottom of this\n",
      "great crack in the earth, and the natural fear that sudden death was\n",
      "about to overtake them at any moment.  Crash after crash echoed far\n",
      "above their heads, as the earth came together where it had split, and\n",
      "stones and chunks of clay rattled around them on every side.  These\n",
      "they could not see, but they could feel them pelting the buggy top, and\n",
      "Jim screamed almost like a human being when a stone overtook him and\n",
      "struck his boney body.  They did not really hurt the poor horse,\n",
      "because everything was falling together; only the stones and rubbish\n",
      "fell faster than the horse and buggy, which were held back by the\n",
      "pressure of the air, so that the terrified animal was actually more\n",
      "frightened than he was injured.\n",
      "\n",
      "How long this state of things continued Dorothy could not even guess,\n",
      "she was so greatly bewildered.  But bye and bye, as she stared ahead\n",
      "into the black chasm with a beating heart, she began to dimly see the\n",
      "form of the horse Jim--his head up in the air, his ears erect and his\n",
      "long legs sprawling in every direction as he tumbled through space.\n",
      "Also, turning her head, she found that she could see the boy beside\n",
      "her, who had until now remained as still and silent as she herself.\n",
      "\n",
      "Dorothy sighed and commenced to breathe easier.  She began to realize\n",
      "that death was not in store for her, after all, but that she had merely\n",
      "started upon another adventure, which promised to be just as queer and\n",
      "unusual as were those she had before encountered.\n",
      "\n",
      "With this thought in mind the girl took heart and leaned her head over\n",
      "the side of the buggy to see where the strange light was coming from.\n",
      "Far below her she found six great glowing balls suspended in the air.\n",
      "The central and largest one was white, and reminded her of the sun.\n",
      "Around it were arranged, like the five points of a star, the other five\n",
      "brilliant balls; one being rose colored, one violet, one yellow, one\n",
      "blue and one orange.  This splendid group of colored suns sent rays\n",
      "darting in every direction, and as the horse and buggy--with Dorothy\n",
      "and Zeb--sank steadily downward and came nearer to the lights, the rays\n",
      "began to take on all the delicate tintings of a rainbow, growing more\n",
      "and more distinct every moment until all the space was brilliantly\n",
      "illuminated.\n",
      "\n",
      "Dorothy was too dazed to say much, but she watched one of Jim's big\n",
      "ears turn to violet and the other to rose, and wondered that his tail\n",
      "should be yellow and his body striped with blue and orange like the\n",
      "stripes of a zebra.  Then she looked at Zeb, whose face was blue and\n",
      "whose hair was pink, and gave a little laugh that sounded a bit nervous.\n",
      "\n",
      "\"Isn't it funny?\" she said.\n",
      "\n",
      "The boy was startled and his eyes were big.  Dorothy had a green streak\n",
      "through the center of her face where the blue and yellow lights came\n",
      "together, and her appearance seemed to add to his fright.\n",
      "\n",
      "\"I--I don't s-s-see any-thing funny--'bout it!\" he stammered.\n",
      "\n",
      "Just then the buggy tipped slowly over upon its side, the body of the\n",
      "horse tipping also.  But they continued to fall, all together, and the\n",
      "boy and girl had no difficulty in remaining upon the seat, just as they\n",
      "were before.  Then they turned bottom side up, and continued to roll\n",
      "slowly over until they were right side up again.  During this time Jim\n",
      "struggled frantically, all his legs kicking the air; but on finding\n",
      "himself in his former position the horse said, in a relieved tone of\n",
      "voice:\n",
      "\n",
      "\"Well, that's better!\"\n",
      "\n",
      "Dorothy and Zeb looked at one another in wonder.\n",
      "\n",
      "\"Can your horse talk?\" she asked.\n",
      "\n",
      "\"Never knew him to, before,\" replied the boy.\n",
      "\n",
      "\"Those were the first words I ever said,\" called out the horse, who had\n",
      "overheard them, \"and I can't explain why I happened to speak then.\n",
      "This is a nice scrape you've got me into, isn't it?\"\n",
      "\n",
      "\"As for that, we are in the same scrape ourselves,\" answered Dorothy,\n",
      "cheerfully.  \"But never mind; something will happen pretty soon.\"\n",
      "\n",
      "\"Of course,\" growled the horse, \"and then we shall be sorry it\n",
      "happened.\"\n",
      "\n",
      "Zeb gave a shiver.  All this was so terrible and unreal that he could\n",
      "not understand it at all, and so had good reason to be afraid.\n",
      "\n",
      "Swiftly they drew near to the flaming colored suns, and passed close\n",
      "beside them.  The light was then so bright that it dazzled their eyes,\n",
      "and they covered their faces with their hands to escape being blinded.\n",
      "There was no heat in the colored suns, however, and after they had\n",
      "passed below them the top of the buggy shut out many of the piercing\n",
      "rays so that the boy and girl could open their eyes again.\n",
      "\n",
      "\"We've got to come to the bottom some time,\" remarked Zeb, with a deep\n",
      "sigh.  \"We can't keep falling forever, you know.\"\n",
      "\n",
      "\"Of course not,\" said Dorothy.  \"We are somewhere in the middle of the\n",
      "earth, and the chances are we'll reach the other side of it before\n",
      "long.  But it's a big hollow, isn't it?\"\n",
      "\n",
      "\"Awful big!\" answered the boy.\n",
      "\n",
      "\"We're coming to something now,\" announced the horse.\n",
      "\n",
      "At this they both put their heads over the side of the buggy and looked\n",
      "down.  Yes; there was land below them; and not so very far away,\n",
      "either.  But they were floating very, very slowly--so slowly that it\n",
      "could no longer be called a fall--and the children had ample time to\n",
      "take heart and look about them.\n",
      "\n",
      "They saw a landscape with mountains and plains, lakes and rivers, very\n",
      "like those upon the earth's surface; but all the scene was splendidly\n",
      "colored by the variegated lights from the six suns.  Here and there\n",
      "were groups of houses that seemed made of clear glass, because they\n",
      "sparkled so brightly.\n",
      "\n",
      "\"I'm sure we are in no danger,\" said Dorothy, in a sober voice.  \"We\n",
      "are falling so slowly that we can't be dashed to pieces when we land,\n",
      "and this country that we are coming to seems quite pretty.\"\n",
      "\n",
      "\"We'll never get home again, though!\" declared Zeb, with a groan.\n",
      "\n",
      "\"Oh, I'm not so sure of that,\" replied the girl.  \"But don't let us\n",
      "worry over such things, Zeb; we can't help ourselves just now, you\n",
      "know, and I've always been told it's foolish to borrow trouble.\"\n",
      "\n",
      "The boy became silent, having no reply to so sensible a speech, and\n",
      "soon both were fully occupied in staring at the strange scenes spread\n",
      "out below them.  They seemed to be falling right into the middle of a\n",
      "big city which had many tall buildings with glass domes and\n",
      "sharp-pointed spires.  These spires were like great spear-points, and\n",
      "if they tumbled upon one of them th\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the number of unique characters — this is your vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encodeded 'hello'\n",
    "# string  to intiger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 61, 68, 68, 71]\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "print (encode('hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode to decocde \"[64, 61, 68, 68, 71] \" to \"hello\"\n",
    "#Intiger to string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "encoded_hello =encode('hello')\n",
    "decoded_hello = decode(encoded_hello)\n",
    "\n",
    "print(decoded_hello)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural networks can't understand raw text like \"The Wizard of Oz\" — they only understand numbers (tensors). \n",
    "# So we need to:Convert characters to integers (encode)\n",
    "#Convert integers back to characters (decode) — useful for generating text later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the entire book text (now a list of integers) into a PyTorch tensor.\n",
    "#Just prints the first 100 encoded characters (as numbers),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([90, 48, 64, 61,  1, 44, 74, 71, 66, 61, 59, 76,  1, 35, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 30, 71, 71, 67,  1, 71, 62,  1, 32, 71, 74, 71,\n",
      "        76, 64, 81,  1, 57, 70, 60,  1, 76, 64, 61,  1, 51, 65, 82, 57, 74, 60,\n",
      "         1, 65, 70,  1, 43, 82,  0,  1,  1,  1,  1,  0, 48, 64, 65, 75,  1, 61,\n",
      "        58, 71, 71, 67,  1, 65, 75,  1, 62, 71, 74,  1, 76, 64, 61,  1, 77, 75,\n",
      "        61,  1, 71, 62,  1, 57, 70, 81, 71, 70])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a demo to show how the model will be trained to predict the next character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: If your text is:\n",
    "\n",
    "\"hello wo\"\n",
    "#Then:\n",
    "\n",
    "x = \"hello w\"\n",
    "\n",
    "y = \"ello wo\"\n",
    "\n",
    "\n",
    "| Input (context) | Target (next char) |\n",
    "| --------------- | ------------------ |\n",
    "| `'h'`           | `'e'`              |\n",
    "| `'he'`          | `'l'`              |\n",
    "| `'hel'`         | `'l'`              |\n",
    "| `'hell'`        | `'o'`              |\n",
    "| `'hello'`       | `' '`              |\n",
    "| `'hello '`      | `'w'`              |\n",
    "| `'hello w'`     | `'o'`              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding Context-Target Pair Creation for Language Modeling (Block Size = 8)\n",
    "#✅ Purpose:\n",
    "#To train a language model, we need to create (input_sequence → next_token) pairs. This code generates those examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([90]) target is tensor(48)\n",
      "when input is tensor([90, 48]) target is tensor(64)\n",
      "when input is tensor([90, 48, 64]) target is tensor(61)\n",
      "when input is tensor([90, 48, 64, 61]) target is tensor(1)\n",
      "when input is tensor([90, 48, 64, 61,  1]) target is tensor(44)\n",
      "when input is tensor([90, 48, 64, 61,  1, 44]) target is tensor(74)\n",
      "when input is tensor([90, 48, 64, 61,  1, 44, 74]) target is tensor(71)\n",
      "when input is tensor([90, 48, 64, 61,  1, 44, 74, 71]) target is tensor(66)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('when input is', context, 'target is', target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Device Selection for Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "# max_iters = 1000\n",
    "# # eval_interval = 2500\n",
    "# learning_rate = 3e-4\n",
    "# eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why use this code with purpose  benefit and real world analogy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.is_available())\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gurudev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:491\u001b[39m, in \u001b[36mget_device_name\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_name\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    480\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[32m    481\u001b[39m \n\u001b[32m    482\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    489\u001b[39m \u001b[33;03m        str: the name of the device\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gurudev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:523\u001b[39m, in \u001b[36mget_device_properties\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_properties\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> _CudaDeviceProperties:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[32m    513\u001b[39m \n\u001b[32m    514\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m \u001b[33;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[32m    524\u001b[39m     device = _get_device_index(device, optional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device >= device_count():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gurudev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    313\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    314\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[73,  1, 54, 72,  1, 58, 75, 58],\n",
      "        [58, 69, 65, 62, 58, 57,  1, 28],\n",
      "        [56, 58, 72, 72, 11,  0,  0,  3],\n",
      "        [26, 74, 73,  1, 33,  1, 54, 66]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[ 1, 54, 72,  1, 58, 75, 58, 71],\n",
      "        [69, 65, 62, 58, 57,  1, 28, 68],\n",
      "        [58, 72, 72, 11,  0,  0,  3, 40],\n",
      "        [74, 73,  1, 33,  1, 54, 66,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "af!KV1E.]S*IWOth\n",
      "P(c8xFL;EBkltv()L]Mqs\"(iUU*2v,k5Mqu!MkS,DR8;3 WNH2MkRJTTa5ji]; NZzgm[RED(3U;-9EN9LMK11M!Y1Dc74bn:MTA?')fcAE9o-E,p&TojQaEgAD0irY\n",
      ", jOPDRjWH\n",
      "(lFT)ga9J9JZR'X!zYufMw [[J﻿tncltGiRtR8VXx71﻿x'RDrpcp7V:OvQTj;3lPP-?6﻿7ke:vN8﻿,ZqcP47K?;.W0gn94K1﻿HR',1YEn:fsh\n",
      "kZynPTTa0Zs tG1E,izU8(w;;n['.ZgFe*zY69h,)tc[)WgDu6_0TNEyampjL'bBV!hgCnb&ULI_PA0S]V]eq:JQjHz:\n",
      "ou﻿EMHR\"(4L\n",
      "ZecMDx\n",
      "k&vMcAEV2RHS;eq2Rj4[)qCpXa'oumKITTnezCY;gDGrS33?weVR!m1Qu94Kb&v,0\"XQt5vJilPlj\n",
      "OIO\"J69\n",
      "w ycv\n",
      "b]2Jdjk.IusN*I)9﻿\n",
      "Kf)Vw5ung5*2\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 2.857, val loss: 2.867\n",
      "step: 250, train loss: 2.835, val loss: 2.860\n",
      "step: 500, train loss: 2.826, val loss: 2.856\n",
      "step: 750, train loss: 2.814, val loss: 2.825\n",
      "3.0060837268829346\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***need to familiarize audience with optimizers (AdamW, Adam, SGD, MSE…) no need to jump into the formulas, just what the optimizer does for us and some of the differences/similarities between them***\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: MSE is a common loss function used in regression problems, where the goal is to predict a continuous output. It measures the average squared difference between the predicted and actual values, and is often used to train neural networks for regression tasks.\n",
    "2. **Gradient Descent (GD):**  is an optimization algorithm used to minimize the loss function of a machine learning model. The loss function measures how well the model is able to predict the target variable based on the input features. The idea of GD is to iteratively adjust the model parameters in the direction of the steepest descent of the loss function\n",
    "3. **Momentum**: Momentum is an extension of SGD that adds a \"momentum\" term to the parameter updates. This term helps smooth out the updates and allows the optimizer to continue moving in the right direction, even if the gradient changes direction or varies in magnitude. Momentum is particularly useful for training deep neural networks.\n",
    "4. **RMSprop**: RMSprop is an optimization algorithm that uses a moving average of the squared gradient to adapt the learning rate of each parameter. This helps to avoid oscillations in the parameter updates and can improve convergence in some cases.\n",
    "5. **Adam**: Adam is a popular optimization algorithm that combines the ideas of momentum and RMSprop. It uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter. Adam is often used as a default optimizer for deep learning models.\n",
    "6. **AdamW**: AdamW is a modification of the Adam optimizer that adds weight decay to the parameter updates. This helps to regularize the model and can improve generalization performance. We will be using the AdamW optimizer as it best suits the properties of the model we will train in this video.\n",
    "\n",
    "find more optimizers and details at torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tesle?uKvmot spY\n",
      "eoyErdB\n",
      "xxfrem[o V8WLDl8Mkm'uz\"Cdd\n",
      "datys?1V!ourM5Cxz&THG8U2K.L_u.HLORuze ls lXtlel8M5J'B&RL)la.O16\n",
      "Tb sL7&or\n",
      "BW\n",
      "\n",
      "\n",
      " ithN:wk\"th!OF :efaSB!!ardgn0kl pcly,[x0W5vSwooyw5.be:WP1k\n",
      ",com﻿eard[\"f7&p9P0U0&Q6HWxR'[sizQ4C4,:\"MK-R6YCvulexk0M5﻿6yEucoatallm.R\"Sb\"KKacop(6-1BS.L:GemsitQ4UL\"magg.QHLG(0azar lPq'bzzgQBL::Bj7jneabM4VSQLar tud. PO5w]uldls8'pgv!BU3T7L*UHPokrriZ3-(. hY38G\n",
      "ZdmpuVJns. dos\"DomD.41,﻿HPPJJ)P24   gqplazsh.9c QyjM5O\n",
      "d,:7Gx3gnaH,\"w1﻿*\"RLpPtWY_﻿,A\n",
      "Srd a, ;diZKImThere?BSmJXwo Qnz\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
